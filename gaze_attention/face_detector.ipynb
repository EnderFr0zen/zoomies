{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2q27gKz1H20"
   },
   "source": [
    "##### Copyright 2023 The MediaPipe Authors. All Rights Reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_cQX8dWu4Dv"
   },
   "source": [
    "# Face Detection with MediaPipe Tasks\n",
    "\n",
    "This notebook shows you how to use the MediaPipe Tasks Python API to detect faces in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6PN9FvIx614"
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Let's start with installing MediaPipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a49D7h4TVmru"
   },
   "source": [
    "Then download an off-the-shelf model. Check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_detector#models) for more face detection models that you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OMjuVQiDYJKF"
   },
   "outputs": [],
   "source": [
    "!wget -q -O detector.tflite -q https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89BlskiiyGDC"
   },
   "source": [
    "## Visualization utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLHhoIkkWYLQ"
   },
   "source": [
    "To better demonstrate the Face Detector API, we have created a set of visualization tools that will be used in this colab. These will draw a bounding box around detected faces, as well as markers over certain detected points on the faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H4aPO-hvbw3r"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Union, List\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import os\n",
    "import urllib.request\n",
    "import time\n",
    "\n",
    "MARGIN = 10  # pixels\n",
    "ROW_SIZE = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "TEXT_COLOR = (255, 0, 0)  # red\n",
    "\n",
    "\n",
    "def _normalized_to_pixel_coordinates(\n",
    "    normalized_x: float, normalized_y: float, image_width: int,\n",
    "    image_height: int) -> Union[None, Tuple[int, int]]:\n",
    "  \"\"\"Converts normalized value pair to pixel coordinates.\"\"\"\n",
    "\n",
    "  # Checks if the float value is between 0 and 1.\n",
    "  def is_valid_normalized_value(value: float) -> bool:\n",
    "    return (value > 0 or math.isclose(0, value)) and (value < 1 or math.isclose(1, value))\n",
    "\n",
    "  if not (is_valid_normalized_value(normalized_x) and\n",
    "          is_valid_normalized_value(normalized_y)):\n",
    "    # TODO: Draw coordinates even if it's outside of the image bounds.\n",
    "    return None\n",
    "  x_px = min(math.floor(normalized_x * image_width), image_width - 1)\n",
    "  y_px = min(math.floor(normalized_y * image_height), image_height - 1)\n",
    "  return x_px, y_px\n",
    "\n",
    "\n",
    "def visualize(\n",
    "    image,\n",
    "    detection_result\n",
    ") -> np.ndarray:\n",
    "  \"\"\"Draws bounding boxes and keypoints on the input image and return it.\n",
    "  Args:\n",
    "    image: The input RGB image.\n",
    "    detection_result: The list of all \"Detection\" entities to be visualize.\n",
    "  Returns:\n",
    "    Image with bounding boxes.\n",
    "  \"\"\"\n",
    "  annotated_image = image.copy()\n",
    "  height, width, _ = image.shape\n",
    "\n",
    "  for detection in detection_result.detections:\n",
    "    # Draw bounding_box\n",
    "    bbox = detection.bounding_box\n",
    "    start_point = bbox.origin_x, bbox.origin_y\n",
    "    end_point = bbox.origin_x + bbox.width, bbox.origin_y + bbox.height\n",
    "    cv2.rectangle(annotated_image, start_point, end_point, TEXT_COLOR, 3)\n",
    "\n",
    "    # Draw keypoints\n",
    "    for keypoint in detection.keypoints:\n",
    "      keypoint_px = _normalized_to_pixel_coordinates(keypoint.x, keypoint.y,\n",
    "                                                     width, height)\n",
    "      color, thickness, radius = (0, 255, 0), 2, 10  # Increased radius from 2 to 10\n",
    "      cv2.circle(annotated_image, keypoint_px, thickness, color, radius)\n",
    "\n",
    "    # Draw label and score\n",
    "    category = detection.categories[0]\n",
    "    category_name = category.category_name\n",
    "    category_name = '' if category_name is None else category_name\n",
    "    probability = round(category.score, 2)\n",
    "    result_text = category_name + ' (' + str(probability) + ')'\n",
    "    text_location = (MARGIN + bbox.origin_x,\n",
    "                     MARGIN + ROW_SIZE + bbox.origin_y)\n",
    "    cv2.putText(annotated_image, result_text, text_location, cv2.FONT_HERSHEY_PLAIN,\n",
    "                FONT_SIZE, TEXT_COLOR, FONT_THICKNESS)\n",
    "\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy4r2_ePylIa"
   },
   "source": [
    "## Running inference and visualizing the results\n",
    "\n",
    "The final step is to run face detection on your selected image. This involves creating your FaceDetector object, loading your image, running detection, and finally, the optional step of displaying the image with visualizations.\n",
    "\n",
    "You can check out the [MediaPipe documentation](https://developers.google.com/mediapipe/solutions/vision/face_detector/python) to learn more about configuration options that this solution supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Face Detection\n",
    "\n",
    "Now let's process video files for face detection. This will process each frame of the video and detect faces in real-time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video processing function\n",
    "def process_video(video_path=0, output_path=None):\n",
    "    \"\"\"\n",
    "    Process video file for face detection\n",
    "    Args:\n",
    "        video_path: Path to input video file\n",
    "        output_path: Path to save output video (optional)\n",
    "    \"\"\"\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    print(f\"Video properties: {width}x{height} @ {fps} FPS\")\n",
    "    \n",
    "    # Setup video writer if output path is provided\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create MediaPipe Image object\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "        \n",
    "        # Detect faces\n",
    "        detection_result = detector.detect(mp_image)\n",
    "        \n",
    "        # Visualize results\n",
    "        annotated_frame = visualize(frame, detection_result)\n",
    "        \n",
    "        # Save frame if output path is provided\n",
    "        if output_path:\n",
    "            out.write(annotated_frame)\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Process every 10th frame to avoid too many displays\n",
    "        if frame_count % 10 == 0:\n",
    "            print(f\"Processed {frame_count} frames\")\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()\n",
    "    \n",
    "    print(f\"Video processing completed. Total frames: {frame_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing live video\n"
     ]
    }
   ],
   "source": [
    "# Process the uploaded video\n",
    "if VIDEO_FILE != 0:\n",
    "    print(\"Processing video for face detection...\")\n",
    "    \n",
    "    # Process video and save output\n",
    "    output_video_path = \"output_with_faces.mp4\"\n",
    "    process_video(VIDEO_FILE, output_video_path)\n",
    "    \n",
    "    print(f\"Output video saved as: {output_video_path}\")\n",
    "else:\n",
    "    print(\"Processing live video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eye Gaze Detection\n",
    "\n",
    "Now let's implement eye gaze detection to determine if the person is looking at the screen/camera. We'll use MediaPipe Face Mesh to detect eye landmarks and calculate gaze direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Face Mesh model\n",
    "!wget -q -O face_landmarker.task https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eye landmark indices for MediaPipe Face Mesh\n",
    "# Left eye landmarks\n",
    "LEFT_EYE_INDICES = [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246]\n",
    "# Right eye landmarks  \n",
    "RIGHT_EYE_INDICES = [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398]\n",
    "\n",
    "# Key eye points for gaze calculation\n",
    "LEFT_EYE_CENTER = 468\n",
    "RIGHT_EYE_CENTER = 473\n",
    "LEFT_EYE_INNER = 133\n",
    "LEFT_EYE_OUTER = 33\n",
    "RIGHT_EYE_INNER = 362\n",
    "RIGHT_EYE_OUTER = 263\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gaze_ratio(eye_landmarks: List[Tuple[float, float]], \n",
    "                        p1_idx: int, \n",
    "                        p2_idx: int,\n",
    "                        center_idx: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate gaze direction ratio with a robust algorithm.\n",
    "    Args:\n",
    "        eye_landmarks: List of all face landmark coordinates.\n",
    "        p1_idx: Index of the first horizontal eye corner.\n",
    "        p2_idx: Index of the second horizontal eye corner.\n",
    "        center_idx: Index of the eye center (iris).\n",
    "    Returns:\n",
    "        Gaze ratio (0.0 = looking left, 1.0 = looking right, 0.5 = looking straight).\n",
    "    \"\"\"\n",
    "    # Extract coordinates for the points of interest\n",
    "    p1 = np.array(eye_landmarks[p1_idx])\n",
    "    p2 = np.array(eye_landmarks[p2_idx])\n",
    "    center = np.array(eye_landmarks[center_idx])\n",
    "    \n",
    "    # Determine the leftmost and rightmost points based on x-coordinate\n",
    "    left_point = p1 if p1[0] < p2[0] else p2\n",
    "    right_point = p2 if p1[0] < p2[0] else p1\n",
    "    \n",
    "    # Calculate total horizontal distance (width of the eye)\n",
    "    eye_width = right_point[0] - left_point[0]\n",
    "    if eye_width == 0:\n",
    "        return 0.5  # Default to center if width is zero\n",
    "\n",
    "    # Calculate horizontal position of the eye center relative to the left corner\n",
    "    center_horizontal_pos = center[0] - left_point[0]\n",
    "    \n",
    "    # Normalize the position to get the ratio\n",
    "    ratio = center_horizontal_pos / eye_width\n",
    "    \n",
    "    # Clip the ratio to handle cases where the iris might be slightly outside the corners\n",
    "    return np.clip(ratio, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eye_aspect_ratio(eye_landmarks: List[Tuple[float, float]]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Eye Aspect Ratio (EAR) to detect if eyes are open\n",
    "    Args:\n",
    "        eye_landmarks: List of eye landmark coordinates\n",
    "    Returns:\n",
    "        EAR value (lower values indicate closed eyes)\n",
    "    \"\"\"\n",
    "    # Use specific eye landmark indices for more accurate calculation\n",
    "    # Top and bottom eye landmarks\n",
    "    top_landmarks = [1, 2]  # Upper eyelid\n",
    "    bottom_landmarks = [4, 5]  # Lower eyelid\n",
    "    horizontal_landmarks = [0, 3]  # Left and right corners\n",
    "    \n",
    "    # Calculate vertical distances\n",
    "    vertical_distances = []\n",
    "    for top_idx in top_landmarks:\n",
    "        for bottom_idx in bottom_landmarks:\n",
    "            if top_idx < len(eye_landmarks) and bottom_idx < len(eye_landmarks):\n",
    "                vertical_dist = np.linalg.norm(\n",
    "                    np.array(eye_landmarks[top_idx]) - np.array(eye_landmarks[bottom_idx])\n",
    "                )\n",
    "                vertical_distances.append(vertical_dist)\n",
    "    \n",
    "    # Calculate horizontal distance\n",
    "    horizontal_dist = 0\n",
    "    if len(horizontal_landmarks) >= 2:\n",
    "        horizontal_dist = np.linalg.norm(\n",
    "            np.array(eye_landmarks[horizontal_landmarks[0]]) - \n",
    "            np.array(eye_landmarks[horizontal_landmarks[1]])\n",
    "        )\n",
    "    \n",
    "    if horizontal_dist == 0:\n",
    "        return 0.3  # Default value if no horizontal distance\n",
    "    \n",
    "    # Calculate EAR\n",
    "    avg_vertical = np.mean(vertical_distances) if vertical_distances else 0\n",
    "    ear = avg_vertical / horizontal_dist\n",
    "    \n",
    "    return ear\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_attention_status(face_landmarks) -> dict:\n",
    "    \"\"\"\n",
    "    Detect if person is looking at screen/camera based on eye gaze.\n",
    "    Args:\n",
    "        face_landmarks: MediaPipe face landmarks.\n",
    "    Returns:\n",
    "        Dictionary with attention status and metrics.\n",
    "    \"\"\"\n",
    "    if not face_landmarks or len(face_landmarks) == 0:\n",
    "        return {\"looking_at_screen\": False, \"confidence\": 0.0, \"gaze_direction\": \"no_face_detected\", \"student_status\": \"Student not looking at the screen\"}\n",
    "    \n",
    "    landmarks = face_landmarks[0]\n",
    "    all_landmarks = [(lm.x, lm.y) for lm in landmarks]\n",
    "    \n",
    "    # Extract eye landmarks for EAR calculation\n",
    "    left_eye_landmarks = [all_landmarks[i] for i in LEFT_EYE_INDICES]\n",
    "    right_eye_landmarks = [all_landmarks[i] for i in RIGHT_EYE_INDICES]\n",
    "    \n",
    "    left_ear = calculate_eye_aspect_ratio(left_eye_landmarks)\n",
    "    right_ear = calculate_eye_aspect_ratio(right_eye_landmarks)\n",
    "    avg_ear = (left_ear + right_ear) / 2.0\n",
    "    \n",
    "    eyes_open = avg_ear > 0.20\n",
    "    \n",
    "    if not eyes_open:\n",
    "        return {\n",
    "            \"looking_at_screen\": False, \n",
    "            \"confidence\": 0.0, \n",
    "            \"gaze_direction\": \"eyes_closed\",\n",
    "            \"ear\": avg_ear,\n",
    "            \"student_status\": \"Student not looking at the screen\"\n",
    "        }\n",
    "    \n",
    "    # Calculate gaze ratios for both eyes using the new robust function\n",
    "    left_gaze_ratio = calculate_gaze_ratio(\n",
    "        all_landmarks, LEFT_EYE_INNER, LEFT_EYE_OUTER, LEFT_EYE_CENTER\n",
    "    )\n",
    "    \n",
    "    # The right eye's inner/outer landmarks are anatomically swapped\n",
    "    # but our new function handles it automatically by finding the min/max x.\n",
    "    right_gaze_ratio = calculate_gaze_ratio(\n",
    "        all_landmarks, RIGHT_EYE_INNER, RIGHT_EYE_OUTER, RIGHT_EYE_CENTER\n",
    "    )\n",
    "    \n",
    "    avg_gaze_ratio = (left_gaze_ratio + right_gaze_ratio) / 2.0\n",
    "    \n",
    "    # Determine gaze direction with adjusted thresholds\n",
    "    if avg_gaze_ratio < 0.4:  # Looking left\n",
    "        gaze_direction = \"looking_left\"\n",
    "    elif avg_gaze_ratio > 0.6: # Looking right\n",
    "        gaze_direction = \"looking_right\"\n",
    "    else:\n",
    "        gaze_direction = \"looking_straight\"\n",
    "    \n",
    "    # Confidence is 1.0 at center (0.5) and 0.0 at the edges (0 or 1)\n",
    "    center_distance = abs(avg_gaze_ratio - 0.5)\n",
    "    confidence = max(0, 1 - center_distance * 2)\n",
    "    \n",
    "    # *** KEY CHANGE HERE ***\n",
    "    # Relax the confidence threshold for looking at the screen\n",
    "    looking_at_screen = confidence >= 0.90 # Was 0.95\n",
    "    \n",
    "    student_status = \"Student looking at the screen\" if looking_at_screen else \"Student not looking at the screen\"\n",
    "    \n",
    "    return {\n",
    "        \"looking_at_screen\": looking_at_screen,\n",
    "        \"confidence\": confidence,\n",
    "        \"gaze_direction\": gaze_direction,\n",
    "        \"gaze_ratio\": avg_gaze_ratio,\n",
    "        \"ear\": avg_ear,\n",
    "        \"eyes_open\": eyes_open,\n",
    "        \"student_status\": student_status\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(image, face_landmarks, attention_status):\n",
    "    \"\"\"\n",
    "    Visualize eye gaze and attention status on image\n",
    "    Args:\n",
    "        image: Input image\n",
    "        face_landmarks: MediaPipe face landmarks\n",
    "        attention_status: Dictionary with attention metrics\n",
    "    Returns:\n",
    "        Annotated image\n",
    "    \"\"\"\n",
    "    annotated_image = image.copy()\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Always show status text, even when no face is detected\n",
    "    if not face_landmarks or len(face_landmarks) == 0:\n",
    "        # Show \"Student not looking at the screen\" message\n",
    "        status_text = \"Student not looking at the screen\"\n",
    "        color = (0, 0, 255)  # Red\n",
    "    else:\n",
    "        landmarks = face_landmarks[0]\n",
    "        \n",
    "        # Draw eye landmarks\n",
    "        for idx in LEFT_EYE_INDICES + RIGHT_EYE_INDICES:\n",
    "            if idx < len(landmarks):\n",
    "                x = int(landmarks[idx].x * width)\n",
    "                y = int(landmarks[idx].y * height)\n",
    "                cv2.circle(annotated_image, (x, y), 2, (0, 255, 0), -1)\n",
    "        \n",
    "        # Draw eye centers\n",
    "        left_center_x = int(landmarks[LEFT_EYE_CENTER].x * width)\n",
    "        left_center_y = int(landmarks[LEFT_EYE_CENTER].y * height)\n",
    "        right_center_x = int(landmarks[RIGHT_EYE_CENTER].x * width)\n",
    "        right_center_y = int(landmarks[RIGHT_EYE_CENTER].y * height)\n",
    "        \n",
    "        cv2.circle(annotated_image, (left_center_x, left_center_y), 5, (255, 0, 0), -1)\n",
    "        cv2.circle(annotated_image, (right_center_x, right_center_y), 5, (255, 0, 0), -1)\n",
    "        \n",
    "        # Prepare status text\n",
    "        status_text = f\"Looking at screen: {'YES' if attention_status['looking_at_screen'] else 'NO'}\"\n",
    "        confidence_text = f\"Confidence: {attention_status['confidence']:.2f}\"\n",
    "        gaze_text = f\"Gaze: {attention_status['gaze_direction']}\"\n",
    "        ear_text = f\"EAR: {attention_status['ear']:.3f}\"\n",
    "        \n",
    "        # Choose colors based on attention status\n",
    "        if attention_status['looking_at_screen']:\n",
    "            color = (0, 255, 0)  # Green\n",
    "        else:\n",
    "            color = (0, 0, 255)  # Red\n",
    "    \n",
    "    # Draw text with background\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1.2  # Increased from 0.7\n",
    "    thickness = 3     # Increased from 2\n",
    "    \n",
    "    # Prepare texts to display\n",
    "    if not face_landmarks or len(face_landmarks) == 0:\n",
    "        texts = [status_text]\n",
    "    else:\n",
    "        texts = [status_text, confidence_text, gaze_text, ear_text]\n",
    "    \n",
    "    y_offset = 30\n",
    "    \n",
    "    for text in texts:\n",
    "        # Get text size for background rectangle\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "        \n",
    "        # Draw background rectangle\n",
    "        cv2.rectangle(annotated_image, \n",
    "                     (10, y_offset - text_height - 5), \n",
    "                     (10 + text_width, y_offset + 5), \n",
    "                     (0, 0, 0), -1)\n",
    "        \n",
    "        # Draw text\n",
    "        cv2.putText(annotated_image, text, (10, y_offset), font, font_scale, color, thickness)\n",
    "        y_offset += 35\n",
    "    \n",
    "    return annotated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759224976.374987 43223734 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1759224976.375534 43223734 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1759224976.378823 43225873 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759224976.384721 43225878 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Face Landmarker\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker.task')\n",
    "options = vision.FaceLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_face_blendshapes=True,\n",
    "    output_facial_transformation_matrixes=True,\n",
    "    num_faces=1\n",
    ")\n",
    "face_landmarker = vision.FaceLandmarker.create_from_options(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time eye gaze detection for video\n",
    "def process_video_with_attention(video_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Process video file for eye gaze detection and attention monitoring\n",
    "    Args:\n",
    "        video_path: Path to input video file\n",
    "        output_path: Path to save output video (optional)\n",
    "    \"\"\"\n",
    "    # Initialize video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(f\"Video properties: {width}x{height} @ {fps:.1f} FPS\")\n",
    "\n",
    "    # Setup video writer if output path is provided\n",
    "    writer = None\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_count = 0\n",
    "    last_timestamp_ms = 0\n",
    "    attention_stats = {\n",
    "        \"total_frames\": 0,\n",
    "        \"looking_at_screen_frames\": 0,\n",
    "        \"eyes_closed_frames\": 0,\n",
    "        \"looking_away_frames\": 0\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Create MediaPipe Image object\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "\n",
    "        if fps > 0:\n",
    "            frame_timestamp_ms = int(frame_count * (1000.0 / fps))\n",
    "        else:\n",
    "            frame_timestamp_ms = last_timestamp_ms + 33\n",
    "        if frame_timestamp_ms <= last_timestamp_ms:\n",
    "            frame_timestamp_ms = last_timestamp_ms + 1\n",
    "        last_timestamp_ms = frame_timestamp_ms\n",
    "\n",
    "        # Detect face landmarks\n",
    "        detection_result = face_landmarker.detect_for_video(mp_image, frame_timestamp_ms)\n",
    "\n",
    "        # Detect attention status\n",
    "        attention_status = detect_attention_status(detection_result.face_landmarks)\n",
    "\n",
    "        # Update statistics\n",
    "        attention_stats[\"total_frames\"] += 1\n",
    "        if attention_status[\"gaze_direction\"] == \"eyes_closed\":\n",
    "            attention_stats[\"eyes_closed_frames\"] += 1\n",
    "        elif attention_status[\"looking_at_screen\"]:\n",
    "            attention_stats[\"looking_at_screen_frames\"] += 1\n",
    "        else:\n",
    "            attention_stats[\"looking_away_frames\"] += 1\n",
    "\n",
    "        # Visualize results\n",
    "        annotated_frame = visualize_attention(frame, detection_result.face_landmarks, attention_status)\n",
    "\n",
    "        # Display frame (local)\n",
    "        cv2.imshow('Attention Detection (Video)', annotated_frame)\n",
    "\n",
    "        # Save frame if output path is provided\n",
    "        if writer:\n",
    "            writer.write(annotated_frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        # Process every 10th frame to log stats\n",
    "        if frame_count % 10 == 0:\n",
    "            print(f\"Processed {frame_count} frames\")\n",
    "            print(f\"Attention stats: {attention_stats}\")\n",
    "\n",
    "        # Handle key events\n",
    "        if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if writer:\n",
    "        writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    total_frames = attention_stats[\"total_frames\"]\n",
    "    if total_frames == 0:\n",
    "        print(\"No frames processed.\")\n",
    "        return attention_stats\n",
    "\n",
    "    # Print final statistics\n",
    "    print(f\"\\nVideo processing completed. Total frames: {frame_count}\")\n",
    "    print(f\"Final attention statistics:\")\n",
    "    print(f\"- Looking at screen: {attention_stats['looking_at_screen_frames']} frames ({attention_stats['looking_at_screen_frames']/total_frames*100:.1f}%)\")\n",
    "    print(f\"- Looking away: {attention_stats['looking_away_frames']} frames ({attention_stats['looking_away_frames']/total_frames*100:.1f}%)\")\n",
    "    print(f\"- Eyes closed: {attention_stats['eyes_closed_frames']} frames ({attention_stats['eyes_closed_frames']/total_frames*100:.1f}%)\")\n",
    "\n",
    "    return attention_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FaceLandmarker initialized (GPU delegate if available).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1759224976.401042 43223734 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.4), renderer: Apple M4\n",
      "W0000 00:00:1759224976.401241 43223734 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "W0000 00:00:1759224976.404621 43225879 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1759224976.410414 43225882 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Face Landmarker with optional GPU delegate\n",
    "MODEL_URL = \"https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\"\n",
    "MODEL_PATH = \"face_landmarker.task\"\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Downloading face_landmarker.task ...\")\n",
    "    urllib.request.urlretrieve(MODEL_URL, MODEL_PATH)\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path=MODEL_PATH)\n",
    "\n",
    "# Try to enable GPU delegate if available; fall back to CPU\n",
    "try:\n",
    "    options = vision.FaceLandmarkerOptions(\n",
    "        base_options=base_options,\n",
    "        running_mode=vision.RunningMode.VIDEO,\n",
    "        num_faces=1,\n",
    "        output_face_blendshapes=True,\n",
    "        output_facial_transformation_matrixes=True\n",
    "    )\n",
    "    face_landmarker = vision.FaceLandmarker.create_from_options(options)\n",
    "    print(\"FaceLandmarker initialized (GPU delegate if available).\")\n",
    "except Exception as e:\n",
    "    print(\"GPU delegate not available or failed, falling back to CPU:\", e)\n",
    "    options = vision.FaceLandmarkerOptions(\n",
    "        base_options=base_options,\n",
    "        running_mode=vision.RunningMode.VIDEO,\n",
    "        num_faces=1\n",
    "    )\n",
    "    face_landmarker = vision.FaceLandmarker.create_from_options(options)\n",
    "    print(\"FaceLandmarker initialized on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera opened: 1920x1080 @ ~15.0 FPS\n",
      "Recording to: attn_recording.mp4\n"
     ]
    }
   ],
   "source": [
    "# Real-time webcam streaming (local)\n",
    "def _open_camera(preferred=(0, 1, 2), backend=cv2.CAP_AVFOUNDATION):\n",
    "    for idx in preferred:\n",
    "        cap = cv2.VideoCapture(idx, backend)\n",
    "        if cap.isOpened():\n",
    "            return cap, idx\n",
    "        cap.release()\n",
    "    raise RuntimeError(\"No available camera (tried: %s)\" % (preferred,))\n",
    "\n",
    "def run_webcam(device_index: int = 0, save_path: str = None):\n",
    "    try:\n",
    "        cap, device_index = _open_camera((device_index, 0, 1, 2))\n",
    "    except RuntimeError as e:\n",
    "        raise RuntimeError(f\"Cannot open camera index {device_index}. {e}\")\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f\"Cannot open camera index {device_index}\")\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(f\"Camera opened: {width}x{height} @ ~{fps:.1f} FPS\")\n",
    "\n",
    "    # Optional recorder\n",
    "    writer = None\n",
    "    if save_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        writer = cv2.VideoWriter(save_path, fourcc, fps, (width, height))\n",
    "        print(f\"Recording to: {save_path}\")\n",
    "\n",
    "    last_timestamp_ms = 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)\n",
    "\n",
    "            timestamp_ms = int(time.monotonic() * 1000)\n",
    "            if timestamp_ms <= last_timestamp_ms:\n",
    "                timestamp_ms = last_timestamp_ms + 1\n",
    "            last_timestamp_ms = timestamp_ms\n",
    "\n",
    "            result = face_landmarker.detect_for_video(mp_image, timestamp_ms)\n",
    "\n",
    "            attention_status = detect_attention_status(result.face_landmarks)\n",
    "            annotated = visualize_attention(frame, result.face_landmarks, attention_status)\n",
    "\n",
    "            cv2.imshow('Attention Detection (Local)', annotated)\n",
    "            if writer:\n",
    "                writer.write(annotated)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == 27:  # ESC to quit\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    finally:\n",
    "        cap.release()\n",
    "        if writer:\n",
    "            writer.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "run_webcam(0, 'attn_recording.mp4')  # pass None to disable recording\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "face-attn-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
